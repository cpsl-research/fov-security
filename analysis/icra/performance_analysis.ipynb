{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform analysis of performance for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# for font embeddings\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams[\"pdf.fonttype\"] = 42\n",
    "matplotlib.rcParams[\"ps.fonttype\"] = 42\n",
    "\n",
    "\n",
    "fig_dir = \"figures\"\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "res_dir = \"results\"\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "\n",
    "SMALL_SIZE = 14\n",
    "MEDIUM_SIZE = 16\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "# rc('font',**{'family':'sans-serif','sans-serif':['Helvetica']})\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rc(\"font\", size=SMALL_SIZE)  # controls default text sizes\n",
    "plt.rc(\"axes\", titlesize=SMALL_SIZE)  # fontsize of the axes title\n",
    "plt.rc(\"axes\", labelsize=MEDIUM_SIZE)  # fontsize of the x and y labels\n",
    "plt.rc(\"xtick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"ytick\", labelsize=SMALL_SIZE)  # fontsize of the tick labels\n",
    "plt.rc(\"legend\", fontsize=SMALL_SIZE)  # legend fontsize\n",
    "plt.rc(\"figure\", titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils import get_datasets, get_models, get_title_mappings\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load in the datasets\n",
    "CSM, datasets = get_datasets(device=device)\n",
    "CDM = CSM.get_scene_dataset_by_index(0)\n",
    "pc_test = CDM.get_lidar(\n",
    "    frame=CDM.get_frames(\"lidar-0\", 0)[0], sensor=\"lidar-0\", agent=0\n",
    ")\n",
    "\n",
    "# load in the models\n",
    "models = get_models(\n",
    "    device=device, cfg_names=[\"unet_adversarial\", \"unet_mc_adversarial\"]\n",
    ")\n",
    "model_titles, adv_titles = get_title_mappings()\n",
    "\n",
    "# set up paths for saving\n",
    "CDM = CSM.get_scene_dataset_by_index(0)\n",
    "pc_test = CDM.get_lidar(\n",
    "    frame=CDM.get_frames(\"lidar-0\", 0)[0], sensor=\"lidar-0\", agent=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from fov.train import BinarySegmentation\n",
    "\n",
    "\n",
    "# preset the dataset dictionary\n",
    "results = defaultdict(list)\n",
    "\n",
    "# loop over data elements\n",
    "n_frames_max = 200\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"Running over {dataset_name} dataset...\")\n",
    "    n_frames_run = min(n_frames_max, len(dataset))\n",
    "    for i, data in tqdm(enumerate(dataset), total=n_frames_run):\n",
    "        if i > n_frames_run:\n",
    "            break\n",
    "        else:\n",
    "            # get data\n",
    "            pc_img, gt_mask = data\n",
    "            pc_img = torch.unsqueeze(pc_img, 0)\n",
    "            pc_np = dataset.get_pointcloud(i)\n",
    "            metadata = dataset.get_metadata(i)\n",
    "\n",
    "            # loop over the models\n",
    "            for model_name, model in models.items():\n",
    "                # inference\n",
    "                pc_img_pred = model(pc_img, pc_np, metadata)\n",
    "\n",
    "                # metrics\n",
    "                metrics = BinarySegmentation.metrics(\n",
    "                    outputs=pc_img_pred,\n",
    "                    labels=gt_mask,\n",
    "                    loss_fn=None,\n",
    "                    threshold=0.7,\n",
    "                    pos_weight=1.0,\n",
    "                    neg_weight=1.0,\n",
    "                    run_auprc=True,\n",
    "                )\n",
    "\n",
    "                # add the results to the logger\n",
    "                results[\"dataset\"].append(dataset_name)\n",
    "                results[\"adv_model\"].append(metadata[\"adv_model\"])\n",
    "                results[\"n_pts_adv\"].append(metadata[\"n_pts_adv\"])\n",
    "                results[\"frame\"].append(metadata[\"frame\"])\n",
    "                results[\"sensor\"].append(metadata[\"sensor\"])\n",
    "                results[\"agent\"].append(metadata[\"agent\"])\n",
    "                results[\"model\"].append(model_name)\n",
    "\n",
    "                # add pretty results specifically for plotting\n",
    "                results[\"Adversary Model\"].append(adv_titles[metadata[\"adv_model\"]])\n",
    "                results[\"Model Name\"].append(model_titles[model_name])\n",
    "\n",
    "                # add quantitative metrics\n",
    "                for k, v in metrics.items():\n",
    "                    if v is not None:\n",
    "                        try:\n",
    "                            try:\n",
    "                                res = v.detach().cpu().item()\n",
    "                            except ValueError:\n",
    "                                res = v.detach().cpu()\n",
    "                            results[k].append(res)\n",
    "                        except AttributeError:\n",
    "                            results[k].append(v)\n",
    "\n",
    "# save the metrics\n",
    "with open(os.path.join(\"results\", \"last_metrics.p\"), \"wb\") as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "\n",
    "# load in the resuls\n",
    "met_path = os.path.join(res_dir, \"last_metrics.p\")\n",
    "if os.path.exists(met_path):\n",
    "    with open(met_path, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "# get the full dataframe\n",
    "df_results = pd.DataFrame.from_dict(results)\n",
    "print(df_results.shape)\n",
    "df_results.head()\n",
    "\n",
    "# perform some mappings on columns to avoid reprocessing everything\n",
    "model_name_mods = {\n",
    "    \"Quantized Ray Trace\": \"Quant. Ray Trace\",\n",
    "    \"Continuous Ray Trace\": \"Cont. Ray Trace\",\n",
    "    \"Concave Hull Polygon\": \"Concave Hull\",\n",
    "    \"MLE UNet + Adv-Train\": \"MLE UNet +\\nAdv-Train\",\n",
    "    \"MCD UNet + Adv-Train\": \"MCD UNet +\\nAdv-Train\",\n",
    "}\n",
    "df_results[\"Model Name\"] = df_results[\"Model Name\"].apply(\n",
    "    lambda x: model_name_mods.get(x, x)\n",
    ")\n",
    "df_results[\"Adversary Model\"] = df_results[\"Adversary Model\"].apply(\n",
    "    lambda x: x.replace(\" Attack\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# violin plot of some of the performance metrics for different attacks\n",
    "import numpy as np\n",
    "\n",
    "save_dir = os.path.join(fig_dir, \"violin\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "metrics_violin = [\"precision\", \"recall\", \"f1\", \"auprc\"]\n",
    "metric_name_xlabel = {\n",
    "    \"precision\": \"Precision   -   TP/(TP+FP)\",\n",
    "    \"recall\": \"Recall   -   TP/(TP+FN)\",\n",
    "    \"f1\": \"F1 Score\",\n",
    "    \"auprc\": \"Area under P/R Curve\",\n",
    "}\n",
    "\n",
    "# --- make a plot for each of the metrics\n",
    "for metric in metrics_violin:\n",
    "    # set up the figure\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "\n",
    "    # downsample the results columns\n",
    "    df_res_sample = df_results.loc[df_results[\"adv_model\"] != \"cluster\"]\n",
    "\n",
    "    # make the plot\n",
    "    g1 = sns.violinplot(\n",
    "        data=df_res_sample,\n",
    "        x=metric,\n",
    "        y=\"Model Name\",\n",
    "        hue=\"Adversary Model\",\n",
    "        split=True,\n",
    "        inner=\"quart\",\n",
    "        gap=0.1,\n",
    "        fill=True,\n",
    "        legend=\"brief\",\n",
    "        dodge=True,\n",
    "        log_scale=False,\n",
    "        density_norm=\"count\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    g1.set(ylabel=None, xlabel=metric_name_xlabel[metric])\n",
    "    g1.set_xlim(left=0.0, right=1.0)\n",
    "    ax.grid(axis=\"x\")\n",
    "\n",
    "    # move the legend\n",
    "    sns.move_legend(\n",
    "        ax,\n",
    "        \"lower center\",\n",
    "        bbox_to_anchor=(0.3, 1),\n",
    "        ncol=3,\n",
    "        title=None,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    # add separating lines between models\n",
    "    for i in range(len(np.unique(df_results[\"Model Name\"])) - 1):\n",
    "        ax.axhline(i + 0.5, color=\"grey\", lw=1)\n",
    "\n",
    "    # save the plot\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, \"violin_{}.{}\".format(metric, \"{}\"))\n",
    "    plt.savefig(save_path.format(\"png\"))\n",
    "    plt.savefig(save_path.format(\"pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fov-security-pWvESQ_k-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
